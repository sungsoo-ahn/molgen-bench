# Transformer Architecture Configuration

architecture:
  type: transformer

  # Model dimensions
  hidden_dim: 512
  num_layers: 8
  num_heads: 8
  ffn_dim: 2048

  # Attention
  attention_type: full  # full, local, sparse
  max_seq_len: 128

  # Positional encoding
  pos_encoding: learnable  # learnable, sinusoidal, none
  use_3d_bias: true  # Add 3D distance bias to attention

  # Normalization
  norm_type: layer
  pre_norm: true

  # Regularization
  dropout: 0.1
  attention_dropout: 0.1

  # Activation
  activation: gelu
